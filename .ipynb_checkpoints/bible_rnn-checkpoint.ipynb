{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23645563-8a41-443d-b7d0-2aec692b4a48",
   "metadata": {},
   "source": [
    "#Bible RNN\n",
    "This is a project testing my understanding of how to implement recurrent neural networks with Pytorch.\n",
    "Based on the relavent chapter in Juratsky's natural language processing textbook, as well as this tutorial: https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "\n",
    "The aim of this project is to train a language model using a recurrent language model over the Gutenberg Corpus King James Bible (though in reality this implementation is text independant) to finish bible verses, or produce new verses in the style of the bible. \n",
    "\n",
    "This model is really bad. It does very poorly at this task.\n",
    "\n",
    "Though it is, categorically, a recurrent neural network language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d548159-48d8-4b2d-b0b5-c7221ce66598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/gris/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "def preprocess_sents(sents):\n",
    "    output = list()\n",
    "    for i in range(len(sents)):\n",
    "        output.append(list())\n",
    "        for j in range(len(sents[i])):\n",
    "            output[-1].append(sents[i][j].lower())\n",
    "    return output\n",
    "\n",
    "def word_list(sents):\n",
    "    words = set()\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            words.add(word.lower())\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "def vectorize_sents(sents, word_to_vect, dict_size, max_len):\n",
    "    vector_sents = list()\n",
    "    i = 0\n",
    "    \n",
    "    for sent in sents:\n",
    "        i+=1\n",
    "        vector_sents.append([[0] * dict_size] * max_len)\n",
    "        for w in range(max_len):\n",
    "            if w < len(sent):\n",
    "                vector_sents[-1][w] =  word_to_vect[sent[w]]\n",
    "        if i % 100 == 0:\n",
    "            print(str(i) + \"/\" + str(len(sents)) + \" Sentences Processed\")\n",
    "    return vector_sents\n",
    "\n",
    "def input_output(sents):\n",
    "    output_sents = list()\n",
    "    input_sents = list()\n",
    "  \n",
    "    for sent in sents:\n",
    "        input_sents.append(sent[:-1])\n",
    "        output_sents.append(sent[1:])\n",
    "    output_sents = torch.FloatTensor(output_sents)\n",
    "    input_sents = torch.FloatTensor(input_sents)\n",
    "    return input_sents, output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef858dd-e69d-4602-b426-9b8d1e6f9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        # RNN Layer\n",
    "        self.soft_max = nn.Softmax(dim=1)\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        out = self.soft_max(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3d5fee-b754-442b-b2c7-5cbe78207d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, word):\n",
    "    model.eval()\n",
    "    out, hidden = model(word)\n",
    "    return out, hidden\n",
    "\n",
    "def sample(model, out_len, start):\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    sent_vects = list()\n",
    "    sent_vects.append(list())\n",
    "    starts = word_tokenize(start.lower())\n",
    "    for word in starts:\n",
    "        if word in word_to_vect.keys():\n",
    "            sent_vects[0].append(word_to_vect[word])\n",
    "        else:\n",
    "            sent_vects[0].append([0]*dict_size)\n",
    "    sent_vects = torch.FloatTensor(sent_vects)\n",
    "    size = out_len - 1\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for i in range(size):\n",
    "        word, h = predict(model, sent_vects)\n",
    "        maxed = [0] * dict_size\n",
    "        maxed[torch.argmax(word[-1]).item()] = 1\n",
    "        maxed = torch.FloatTensor(maxed)\n",
    "        reshaped_word = torch.reshape(maxed, (1, 1, dict_size)) \n",
    "        \n",
    "        sent_vects = torch.cat((sent_vects, reshaped_word), 1)\n",
    "    out_sent = list()\n",
    "    for word in sent_vects[0]:\n",
    "        out_sent.append(bible_words[torch.argmax(word).item()])\n",
    "    return out_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4156fe-0653-4947-b51c-c7ef0664aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 Sentences Processed\n",
      "Preprocessing Complete\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize dataset\n",
    "num_sents = 100\n",
    "gut_sents = gutenberg.sents('bible-kjv.txt')[:num_sents]\n",
    "\n",
    "#process dataset into usable form\n",
    "bible_sents = preprocess_sents(gut_sents)\n",
    "\n",
    "#key metadata\n",
    "bible_words = word_list(bible_sents)\n",
    "dict_size = len(bible_words)\n",
    "max_len = len(max(bible_sents, key=len))\n",
    "\n",
    "#my choice of embedding\n",
    "#dictionary to read embeddings later\n",
    "word_to_vect = dict()\n",
    "for i in range(dict_size):\n",
    "    word_to_vect[bible_words[i]] = [0] * dict_size\n",
    "    word_to_vect[bible_words[i]][i] = 1\n",
    "#transform into input and output\n",
    "#specific form from tutorial\n",
    "#https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "vect_sents = vectorize_sents(bible_sents, word_to_vect, dict_size, max_len)\n",
    "input_sents, output_sents = input_output(vect_sents)\n",
    "print('Preprocessing Complete\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0aa94-630b-4aaa-8769-524a2147d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "---------------\n",
      "Epoch: 2/300............. Loss: 1.4725\n",
      "Epoch: 4/300............. Loss: 1.4725\n",
      "Epoch: 6/300............. Loss: 1.4725\n",
      "Epoch: 8/300............. Loss: 1.4725\n",
      "Epoch: 10/300............. Loss: 1.4724\n",
      "Epoch: 12/300............. Loss: 1.4723\n",
      "Epoch: 14/300............. Loss: 1.4721\n",
      "Epoch: 16/300............. Loss: 1.4717\n",
      "Epoch: 18/300............. Loss: 1.4711\n",
      "Epoch: 20/300............. Loss: 1.4703\n",
      "Epoch: 22/300............. Loss: 1.4692\n",
      "Epoch: 24/300............. Loss: 1.4677\n",
      "Epoch: 26/300............. Loss: 1.4660\n",
      "Epoch: 28/300............. Loss: 1.4640\n",
      "Epoch: 30/300............. Loss: 1.4620\n",
      "Epoch: 32/300............. Loss: 1.4601\n",
      "Epoch: 34/300............. Loss: 1.4585\n",
      "Epoch: 36/300............. Loss: 1.4573\n",
      "Epoch: 38/300............. Loss: 1.4564\n",
      "Epoch: 40/300............. Loss: 1.4556\n",
      "Epoch: 42/300............. Loss: 1.4549\n",
      "Epoch: 44/300............. Loss: 1.4543\n",
      "Epoch: 46/300............. Loss: 1.4539\n",
      "Epoch: 48/300............. Loss: 1.4535\n",
      "Epoch: 50/300............. Loss: 1.4532\n",
      "Epoch: 52/300............. Loss: 1.4530\n",
      "Epoch: 54/300............. Loss: 1.4529\n",
      "Epoch: 56/300............. Loss: 1.4528\n",
      "Epoch: 58/300............. Loss: 1.4526\n",
      "Epoch: 60/300............. Loss: 1.4525\n",
      "Epoch: 62/300............. Loss: 1.4523\n",
      "Epoch: 64/300............. Loss: 1.4521\n",
      "Epoch: 66/300............. Loss: 1.4518\n",
      "Epoch: 68/300............. Loss: 1.4513\n",
      "Epoch: 70/300............. Loss: 1.4508\n",
      "Epoch: 72/300............. Loss: 1.4504\n",
      "Epoch: 74/300............. Loss: 1.4502\n",
      "Epoch: 76/300............. Loss: 1.4499\n",
      "Epoch: 78/300............. Loss: 1.4496\n",
      "Epoch: 80/300............. Loss: 1.4492\n",
      "Epoch: 82/300............. Loss: 1.4489\n",
      "Epoch: 84/300............. Loss: 1.4486\n",
      "Epoch: 86/300............. Loss: 1.4484\n",
      "Epoch: 88/300............. Loss: 1.4482\n",
      "Epoch: 90/300............. Loss: 1.4480\n",
      "Epoch: 92/300............. Loss: 1.4478\n",
      "Epoch: 94/300............. Loss: 1.4475\n",
      "Epoch: 96/300............. Loss: 1.4471\n",
      "Epoch: 98/300............. Loss: 1.4467\n",
      "Epoch: 100/300............. Loss: 1.4461\n",
      "Epoch: 102/300............. Loss: 1.4454\n",
      "Epoch: 104/300............. Loss: 1.4448\n",
      "Epoch: 106/300............. Loss: 1.4441\n",
      "Epoch: 108/300............. Loss: 1.4435\n",
      "Epoch: 110/300............. Loss: 1.4429\n",
      "Epoch: 112/300............. Loss: 1.4423\n",
      "Epoch: 114/300............. Loss: 1.4417\n",
      "Epoch: 116/300............. Loss: 1.4411\n",
      "Epoch: 118/300............. Loss: 1.4405\n",
      "Epoch: 120/300............. Loss: 1.4399\n",
      "Epoch: 122/300............. Loss: 1.4394\n",
      "Epoch: 124/300............. Loss: 1.4389\n",
      "Epoch: 126/300............. Loss: 1.4385\n",
      "Epoch: 128/300............. Loss: 1.4381\n",
      "Epoch: 130/300............. Loss: 1.4378\n",
      "Epoch: 132/300............. Loss: 1.4375\n",
      "Epoch: 134/300............. Loss: 1.4372\n",
      "Epoch: 136/300............. Loss: 1.4370\n",
      "Epoch: 138/300............. Loss: 1.4368\n",
      "Epoch: 140/300............. Loss: 1.4366\n",
      "Epoch: 142/300............. Loss: 1.4364\n",
      "Epoch: 144/300............. Loss: 1.4363\n",
      "Epoch: 146/300............. Loss: 1.4361\n",
      "Epoch: 148/300............. Loss: 1.4360\n",
      "Epoch: 150/300............. Loss: 1.4359\n",
      "Epoch: 152/300............. Loss: 1.4358\n",
      "Epoch: 154/300............. Loss: 1.4357\n",
      "Epoch: 156/300............. Loss: 1.4356\n",
      "Epoch: 158/300............. Loss: 1.4356\n",
      "Epoch: 160/300............. Loss: 1.4355\n",
      "Epoch: 162/300............. Loss: 1.4354\n",
      "Epoch: 164/300............. Loss: 1.4354\n",
      "Epoch: 166/300............. Loss: 1.4353\n",
      "Epoch: 168/300............. Loss: 1.4353\n",
      "Epoch: 170/300............. Loss: 1.4352\n",
      "Epoch: 172/300............. Loss: 1.4352\n",
      "Epoch: 174/300............. Loss: 1.4351\n",
      "Epoch: 176/300............. Loss: 1.4351\n",
      "Epoch: 178/300............. Loss: 1.4350\n",
      "Epoch: 180/300............. Loss: 1.4350\n",
      "Epoch: 182/300............. Loss: 1.4349\n",
      "Epoch: 184/300............. Loss: 1.4349\n",
      "Epoch: 186/300............. Loss: 1.4349\n",
      "Epoch: 188/300............. Loss: 1.4348\n",
      "Epoch: 190/300............. Loss: 1.4348\n",
      "Epoch: 192/300............. Loss: 1.4347\n",
      "Epoch: 194/300............. Loss: 1.4347\n",
      "Epoch: 196/300............. Loss: 1.4347\n",
      "Epoch: 198/300............. Loss: 1.4346\n",
      "Epoch: 200/300............. Loss: 1.4346\n",
      "Epoch: 202/300............. Loss: 1.4345\n",
      "Epoch: 204/300............. Loss: 1.4345\n",
      "Epoch: 206/300............. Loss: 1.4344\n",
      "Epoch: 208/300............. Loss: 1.4343\n",
      "Epoch: 210/300............. Loss: 1.4342\n",
      "Epoch: 212/300............. Loss: 1.4341\n",
      "Epoch: 214/300............. Loss: 1.4339\n",
      "Epoch: 216/300............. Loss: 1.4337\n",
      "Epoch: 218/300............. Loss: 1.4334\n",
      "Epoch: 220/300............. Loss: 1.4330\n",
      "Epoch: 222/300............. Loss: 1.4325\n",
      "Epoch: 224/300............. Loss: 1.4318\n",
      "Epoch: 226/300............. Loss: 1.4308\n",
      "Epoch: 228/300............. Loss: 1.4295\n",
      "Epoch: 230/300............. Loss: 1.4279\n",
      "Epoch: 232/300............. Loss: 1.4264\n",
      "Epoch: 234/300............. Loss: 1.4247\n",
      "Epoch: 236/300............. Loss: 1.4232\n",
      "Epoch: 238/300............. Loss: 1.4222\n",
      "Epoch: 240/300............. Loss: 1.4213\n",
      "Epoch: 242/300............. Loss: 1.4204\n",
      "Epoch: 244/300............. Loss: 1.4198\n",
      "Epoch: 246/300............. Loss: 1.4193\n",
      "Epoch: 248/300............. Loss: 1.4188\n",
      "Epoch: 250/300............. Loss: 1.4184\n",
      "Epoch: 252/300............. Loss: 1.4180\n",
      "Epoch: 254/300............. Loss: 1.4176\n",
      "Epoch: 256/300............. Loss: 1.4173\n",
      "Epoch: 258/300............. Loss: 1.4170\n",
      "Epoch: 260/300............. Loss: 1.4167\n",
      "Epoch: 262/300............. Loss: 1.4165\n",
      "Epoch: 264/300............. Loss: 1.4162\n",
      "Epoch: 266/300............. Loss: 1.4160\n",
      "Epoch: 268/300............. Loss: 1.4158\n",
      "Epoch: 270/300............. Loss: 1.4156\n",
      "Epoch: 272/300............. Loss: 1.4154\n",
      "Epoch: 274/300............. Loss: 1.4152\n",
      "Epoch: 276/300............. Loss: 1.4151\n",
      "Epoch: 278/300............. Loss: 1.4149\n",
      "Epoch: 280/300............. Loss: 1.4148\n",
      "Epoch: 282/300............. Loss: 1.4147\n",
      "Epoch: 284/300............. Loss: 1.4145\n",
      "Epoch: 286/300............. Loss: 1.4144\n",
      "Epoch: 288/300............. Loss: 1.4143\n",
      "Epoch: 290/300............. Loss: 1.4143\n",
      "Epoch: 292/300............. Loss: 1.4142\n",
      "Epoch: 294/300............. Loss: 1.4141\n",
      "Epoch: 296/300............. Loss: 1.4140\n",
      "Epoch: 298/300............. Loss: 1.4139\n",
      "Epoch: 300/300............. Loss: 1.4138\n",
      "Evaluating Model\n",
      "Enter start of sentence: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " the king\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter length of sentence: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'king', '13', 'and', 'the']\n",
      "Enter start of sentence: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " for\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter length of sentence: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', ':']\n",
      "Enter start of sentence: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " then he said\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter length of sentence: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['then', 'he', 'said', 'and', 'the', 'the']\n",
      "Enter start of sentence: \n"
     ]
    }
   ],
   "source": [
    "#super parameters\n",
    "hidden_dim = 100\n",
    "n_layers = 1\n",
    "model = Model(dict_size, dict_size, hidden_dim, n_layers)\n",
    "n_epochs = 300\n",
    "lr=.001\n",
    "\n",
    "#following juratsky textbook\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Model\\n---------------\")\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    sent_i = 0\n",
    "    output, hidden = model(input_sents)\n",
    "    loss = criterion(output, output_sents.view(-1, dict_size))\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    if epoch%2 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "print(\"Evaluating Model\")\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    print(\"Enter start of sentence: \" )\n",
    "    start = input()\n",
    "    print(\"Enter length of sentence: \")\n",
    "    length = input()\n",
    "    print(sample(model, int(length), start)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5fd26-38c1-434b-9a52-a06d8559816b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
